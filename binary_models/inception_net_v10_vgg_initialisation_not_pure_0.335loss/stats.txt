model_input = Input(shape=image_shape)

# Feature indentification and reduction
feature_extraction_layer = Conv2D(32, (3,3), activation='relu', padding='same')(model_input)
feature_extraction_layer = Conv2D(32, (3,3), activation='relu', padding='same')(feature_extraction_layer)
feature_extraction_layer = BatchNormalization()(feature_extraction_layer)

feature_extraction_layer = Conv2D(64, (3,3), activation='relu')(feature_extraction_layer)
feature_extraction_layer = Conv2D(64, (3,3), activation='relu')(feature_extraction_layer)
feature_extraction_layer = BatchNormalization()(feature_extraction_layer)

feature_extraction_layer = Conv2D(96, (3,3), activation='relu')(feature_extraction_layer)
feature_extraction_layer = Conv2D(96, (3,3), activation='relu')(feature_extraction_layer)
feature_extraction_layer = BatchNormalization()(feature_extraction_layer)

a1 = Conv2D(96, (3,3), activation='relu', padding='same', strides=(2,2))(feature_extraction_layer)

a2 = MaxPooling2D((2,2))(feature_extraction_layer)

feature_reduction_layer = concatenate([a1, a2])

y1 = Conv2D(128, (3,3), activation='relu', padding='same', strides=(2,2))(feature_reduction_layer)

y2 = Conv2D(128, (1,1), activation='relu')(feature_reduction_layer)
y2 = MaxPooling2D((2,2))(y2)

second_reduction_layer = concatenate([y1, y2])

x1 = Conv2D(128, (1,1), activation='relu')(second_reduction_layer)

x2 = Conv2D(128, (1,1), activation='relu')(second_reduction_layer)
x2 = Conv2D(96, (3,3), activation='relu', padding='same')(x2)

x3 = Conv2D(128, (1,1), activation='relu')(second_reduction_layer)
x3 = Conv2D(96, (5,5), activation='relu', padding='same')(x3)

x4 = MaxPooling2D((3,3), strides=(1,1), padding='same')(second_reduction_layer)
x4 = Conv2D(128, (1,1), activation='relu')(x4)

inception_layer_1 = concatenate([x1, x2, x3, x4])

z1 = DepthwiseConv2D((3,3), activation='relu', padding='same')(inception_layer_1)
z1 = DepthwiseConv2D((3,3), activation='relu', padding='same', strides=(2,2))(z1)
z1 = BatchNormalization()(z1)

z2 = Conv2D(320, (1,1), activation='relu')(inception_layer_1)
z2 = MaxPooling2D((2,2))(z2)

final_reduction_layer = concatenate([z1, z2])

model = AveragePooling2D((15,15))(final_reduction_layer)


# model = SeparableConv2D(128, (3,3), activation='relu')(model)
# model = BatchNormalization()(model)

# model = SeparableConv2D(96, (3,3), activation='relu')(model)
# model = BatchNormalization()(model)

# model = MaxPooling2D((2,2))(model)
# model = SeparableConv2D(48, (1,1), activation='relu')(model)

model = Flatten()(model)

model = Dense(128, activation='relu')(model)

model = BatchNormalization()(model)

# model = Dense(64, activation='relu')(model)

# model = BatchNormalization()(model)

model_output = Dense(1, activation='sigmoid')(model)

model = Model(inputs=model_input, outputs=model_output)


model.compile(loss='binary_crossentropy',
              optimizer='RMSProp',
              metrics=['accuracy'])

Model: "model_13"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_19 (InputLayer)           [(None, 128, 128, 1) 0                                            
__________________________________________________________________________________________________
conv2d_221 (Conv2D)             (None, 128, 128, 32) 320         input_19[0][0]                   
__________________________________________________________________________________________________
conv2d_222 (Conv2D)             (None, 128, 128, 32) 9248        conv2d_221[0][0]                 
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 128, 128, 32) 128         conv2d_222[0][0]                 
__________________________________________________________________________________________________
conv2d_223 (Conv2D)             (None, 126, 126, 64) 18496       batch_normalization_69[0][0]     
__________________________________________________________________________________________________
conv2d_224 (Conv2D)             (None, 124, 124, 64) 36928       conv2d_223[0][0]                 
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 124, 124, 64) 256         conv2d_224[0][0]                 
__________________________________________________________________________________________________
conv2d_225 (Conv2D)             (None, 122, 122, 96) 55392       batch_normalization_70[0][0]     
__________________________________________________________________________________________________
conv2d_226 (Conv2D)             (None, 120, 120, 96) 83040       conv2d_225[0][0]                 
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 120, 120, 96) 384         conv2d_226[0][0]                 
__________________________________________________________________________________________________
conv2d_227 (Conv2D)             (None, 60, 60, 96)   83040       batch_normalization_71[0][0]     
__________________________________________________________________________________________________
max_pooling2d_67 (MaxPooling2D) (None, 60, 60, 96)   0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 60, 60, 192)  0           conv2d_227[0][0]                 
                                                                 max_pooling2d_67[0][0]           
__________________________________________________________________________________________________
conv2d_229 (Conv2D)             (None, 60, 60, 128)  24704       concatenate_67[0][0]             
__________________________________________________________________________________________________
conv2d_228 (Conv2D)             (None, 30, 30, 128)  221312      concatenate_67[0][0]             
__________________________________________________________________________________________________
max_pooling2d_68 (MaxPooling2D) (None, 30, 30, 128)  0           conv2d_229[0][0]                 
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 30, 30, 256)  0           conv2d_228[0][0]                 
                                                                 max_pooling2d_68[0][0]           
__________________________________________________________________________________________________
conv2d_231 (Conv2D)             (None, 30, 30, 128)  32896       concatenate_68[0][0]             
__________________________________________________________________________________________________
conv2d_233 (Conv2D)             (None, 30, 30, 128)  32896       concatenate_68[0][0]             
__________________________________________________________________________________________________
max_pooling2d_69 (MaxPooling2D) (None, 30, 30, 256)  0           concatenate_68[0][0]             
__________________________________________________________________________________________________
conv2d_230 (Conv2D)             (None, 30, 30, 128)  32896       concatenate_68[0][0]             
__________________________________________________________________________________________________
conv2d_232 (Conv2D)             (None, 30, 30, 96)   110688      conv2d_231[0][0]                 
__________________________________________________________________________________________________
conv2d_234 (Conv2D)             (None, 30, 30, 96)   307296      conv2d_233[0][0]                 
__________________________________________________________________________________________________
conv2d_235 (Conv2D)             (None, 30, 30, 128)  32896       max_pooling2d_69[0][0]           
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 30, 30, 448)  0           conv2d_230[0][0]                 
                                                                 conv2d_232[0][0]                 
                                                                 conv2d_234[0][0]                 
                                                                 conv2d_235[0][0]                 
__________________________________________________________________________________________________
depthwise_conv2d_34 (DepthwiseC (None, 30, 30, 448)  4480        concatenate_69[0][0]             
__________________________________________________________________________________________________
depthwise_conv2d_35 (DepthwiseC (None, 15, 15, 448)  4480        depthwise_conv2d_34[0][0]        
__________________________________________________________________________________________________
conv2d_236 (Conv2D)             (None, 30, 30, 320)  143680      concatenate_69[0][0]             
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 15, 15, 448)  1792        depthwise_conv2d_35[0][0]        
__________________________________________________________________________________________________
max_pooling2d_70 (MaxPooling2D) (None, 15, 15, 320)  0           conv2d_236[0][0]                 
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 15, 15, 768)  0           batch_normalization_72[0][0]     
                                                                 max_pooling2d_70[0][0]           
__________________________________________________________________________________________________
average_pooling2d_14 (AveragePo (None, 1, 1, 768)    0           concatenate_70[0][0]             
__________________________________________________________________________________________________
flatten_13 (Flatten)            (None, 768)          0           average_pooling2d_14[0][0]       
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 128)          98432       flatten_13[0][0]                 
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 128)          512         dense_31[0][0]                   
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 1)            129         batch_normalization_73[0][0]     
==================================================================================================
Total params: 1,336,321
Trainable params: 1,334,785
Non-trainable params: 1,536
__________________________________________________________________________________________________

WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
WARNING:tensorflow:sample_weight modes were coerced from
  ...
    to  
  ['...']
Train for 326 steps, validate for 39 steps
Epoch 1/30
326/326 [==============================] - 59s 182ms/step - loss: 0.3787 - accuracy: 0.8319 - val_loss: 1.0793 - val_accuracy: 0.5609
Epoch 2/30
326/326 [==============================] - 59s 180ms/step - loss: 0.2955 - accuracy: 0.8723 - val_loss: 1.0828 - val_accuracy: 0.6250
Epoch 3/30
326/326 [==============================] - 59s 182ms/step - loss: 0.2584 - accuracy: 0.8926 - val_loss: 0.9412 - val_accuracy: 0.7099
Epoch 4/30
326/326 [==============================] - 59s 180ms/step - loss: 0.2391 - accuracy: 0.8990 - val_loss: 2.3283 - val_accuracy: 0.6250
Epoch 5/30
326/326 [==============================] - 59s 179ms/step - loss: 0.2318 - accuracy: 0.9057 - val_loss: 1.2613 - val_accuracy: 0.5096
Epoch 6/30
326/326 [==============================] - 58s 178ms/step - loss: 0.2206 - accuracy: 0.9107 - val_loss: 0.9446 - val_accuracy: 0.6587
Epoch 7/30
326/326 [==============================] - 57s 176ms/step - loss: 0.2123 - accuracy: 0.9141 - val_loss: 1.3202 - val_accuracy: 0.4231
Epoch 8/30
326/326 [==============================] - 57s 175ms/step - loss: 0.2063 - accuracy: 0.9191 - val_loss: 0.9026 - val_accuracy: 0.6859
Epoch 9/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1894 - accuracy: 0.9262 - val_loss: 237.7983 - val_accuracy: 0.6250
Epoch 10/30
326/326 [==============================] - 57s 173ms/step - loss: 0.1902 - accuracy: 0.9216 - val_loss: 3.7902 - val_accuracy: 0.5673
Epoch 11/30
326/326 [==============================] - 59s 181ms/step - loss: 0.1815 - accuracy: 0.9314 - val_loss: 2.7781 - val_accuracy: 0.6250
Epoch 12/30
326/326 [==============================] - 57s 176ms/step - loss: 0.1831 - accuracy: 0.9287 - val_loss: 1.0099 - val_accuracy: 0.7083
Epoch 13/30
326/326 [==============================] - 58s 177ms/step - loss: 0.1722 - accuracy: 0.9333 - val_loss: 155.6122 - val_accuracy: 0.4936
Epoch 14/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1696 - accuracy: 0.9333 - val_loss: 4.4674 - val_accuracy: 0.3798
Epoch 15/30
326/326 [==============================] - 56s 173ms/step - loss: 0.1600 - accuracy: 0.9396 - val_loss: 1.5769 - val_accuracy: 0.6330
Epoch 16/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1634 - accuracy: 0.9358 - val_loss: 3.9531 - val_accuracy: 0.6250
Epoch 17/30
326/326 [==============================] - 56s 173ms/step - loss: 0.1596 - accuracy: 0.9385 - val_loss: 0.8974 - val_accuracy: 0.7276
Epoch 18/30
326/326 [==============================] - 57s 175ms/step - loss: 0.1527 - accuracy: 0.9408 - val_loss: 3.7213 - val_accuracy: 0.3734
Epoch 19/30
326/326 [==============================] - 57s 176ms/step - loss: 0.1528 - accuracy: 0.9425 - val_loss: 0.4863 - val_accuracy: 0.8237
Epoch 20/30
326/326 [==============================] - 58s 178ms/step - loss: 0.1484 - accuracy: 0.9450 - val_loss: 0.5131 - val_accuracy: 0.8093
Epoch 21/30
326/326 [==============================] - 58s 177ms/step - loss: 0.1453 - accuracy: 0.9431 - val_loss: 1.4513 - val_accuracy: 0.6346
Epoch 22/30
326/326 [==============================] - 57s 176ms/step - loss: 0.1379 - accuracy: 0.9457 - val_loss: 0.6553 - val_accuracy: 0.7388
Epoch 23/30
326/326 [==============================] - 58s 178ms/step - loss: 0.1395 - accuracy: 0.9496 - val_loss: 0.3738 - val_accuracy: 0.8542
Epoch 24/30
326/326 [==============================] - 57s 176ms/step - loss: 0.1298 - accuracy: 0.9505 - val_loss: 0.3460 - val_accuracy: 0.8462
Epoch 25/30
326/326 [==============================] - 57s 176ms/step - loss: 0.1291 - accuracy: 0.9507 - val_loss: 0.8302 - val_accuracy: 0.7821
Epoch 26/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1287 - accuracy: 0.9507 - val_loss: 0.9358 - val_accuracy: 0.6699
Epoch 27/30
326/326 [==============================] - 56s 173ms/step - loss: 0.1255 - accuracy: 0.9525 - val_loss: 0.3468 - val_accuracy: 0.8654
Epoch 28/30
326/326 [==============================] - 57s 173ms/step - loss: 0.1314 - accuracy: 0.9515 - val_loss: 0.3462 - val_accuracy: 0.8478
Epoch 29/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1286 - accuracy: 0.9544 - val_loss: 0.3355 - val_accuracy: 0.8702
Epoch 30/30
326/326 [==============================] - 57s 174ms/step - loss: 0.1272 - accuracy: 0.9551 - val_loss: 0.3457 - val_accuracy: 0.8670